{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6uZ7yHoq4xNs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "import scipy.io\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BmyYU49S8aB4"
      },
      "outputs": [],
      "source": [
        "class Self_Attention(nn.Module):\n",
        "    def __init__(self, in_channels,out_channels):\n",
        "        super(Self_Attention, self).__init__()\n",
        "        self.query_matrix = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.key_matrix = nn.Conv2d(in_channels, out_channels , kernel_size=1)\n",
        "        self.value_matrix = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.final_conv=nn.Conv2d(out_channels, in_channels, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, height, width = x.size()\n",
        "        query_vec=self.query_matrix(x)\n",
        "        query_vec=query_vec.view(batch_size,-1,height*width)\n",
        "        key_vec =self.key_matrix(x)\n",
        "        key_vec = key_vec.view(batch_size,-1,height*width)\n",
        "        value_vec =self.value_matrix(x)\n",
        "        value_vec = value_vec.view(batch_size,-1,height*width)\n",
        "        similarities=torch.bmm(query_vec.permute(0,2,1),key_vec)\n",
        "        attention_scores=F.softmax(similarities,dim=1)\n",
        "        output=torch.bmm(value_vec,attention_scores)\n",
        "        output=output.view(batch_size,-1,height,width)\n",
        "        output=self.final_conv(output)\n",
        "        output=self.gamma*output+x\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mdKjiuq48dVZ"
      },
      "outputs": [],
      "source": [
        "class CNNWithAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNWithAttention, self).__init__()\n",
        "        self.conv_layer1 = nn.Conv2d(3, 32, kernel_size=3, padding=1,stride=3)\n",
        "        self.self_atten_layer1 = Self_Attention(32,4)\n",
        "        self.conv_layer2 = nn.Conv2d(32, 64, kernel_size=3, padding=1,stride=3)\n",
        "        self.self_atten_layer2 = Self_Attention(64,16)\n",
        "        self.conv_layer3 = nn.Conv2d(64, 128, kernel_size=3, padding=1,stride=3)\n",
        "        self.self_atten_layer3 = Self_Attention(128,32)\n",
        "        self.conv_layer4 = nn.Conv2d(128, 256, kernel_size=3, padding=1,stride=3)\n",
        "        self.self_atten_layer4 = Self_Attention(256,64)\n",
        "        self.conv_layer5 = nn.Conv2d(256, 10, kernel_size=3, padding=1,stride=3)\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "    def forward(self, x):\n",
        "        x=self.conv_layer1(x)\n",
        "        x=self.self_atten_layer1(x)\n",
        "        x=self.conv_layer2(x)\n",
        "        x=self.self_atten_layer2(x)\n",
        "        x=self.conv_layer3(x)\n",
        "        x=self.self_atten_layer3(x)\n",
        "        x=self.conv_layer4(x)\n",
        "        x=self.self_atten_layer4(x)\n",
        "        x=self.conv_layer5(x)\n",
        "        x=self.global_avg_pool(x)\n",
        "        x = x.view(-1,10 )\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "huzXoRx48jJ-"
      },
      "outputs": [],
      "source": [
        "# Load .mat data from Google Drive\n",
        "train_data_path = '/content/drive/MyDrive/Colab Notebooks/cifar-10-batches-mat/data_batch_1.mat'\n",
        "mat_data = scipy.io.loadmat(train_data_path)\n",
        "\n",
        "# Extract relevant data and reshape\n",
        "train_data_X = torch.tensor(mat_data[\"data\"]).reshape(-1, 3, 32, 32).float() / 255.0  # Reshape and normalize\n",
        "train_data_Y = torch.tensor(mat_data[\"labels\"]).squeeze()  # Squeeze to remove extra dimension\n",
        "\n",
        "# Create TensorDataset\n",
        "train_dataset = TensorDataset(train_data_X, train_data_Y)\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 100  # Adjust batch size as needed\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# train_loader = train_loader.to('cuda')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIL4KDovIyAE",
        "outputId": "4a6663f3-710d-4e3e-f113-a6d45df0b3a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.9204156398773193\n",
            "1.9103448390960693\n",
            "1.764265537261963\n",
            "1.6732937097549438\n",
            "1.5931392908096313\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the model\n",
        "model = CNNWithAttention()\n",
        "# model = model.to('cuda')\n",
        "# Define your loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "tot_loss=0\n",
        "# Training loop\n",
        "for epoch in range(5):  # Adjust the number of epochs as needed\n",
        "    model.train()\n",
        "    tot_loss=0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # data, target = data.to('cuda'), target.to('cuda')\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        tot_loss=+loss.item()\n",
        "        # print(batch_idx,\" batch loss:-\",loss.item())\n",
        "    print(tot_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRaQtQyB8Tk4",
        "outputId": "f655073d-4224-4849-bc2d-c9e7bc036dd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10000, 3, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# Load .mat data from Google Drive\n",
        "test_data_path = '/content/drive/MyDrive/Colab Notebooks/cifar-10-batches-mat/test_batch.mat'\n",
        "test_mat_data = scipy.io.loadmat(test_data_path)\n",
        "\n",
        "\n",
        "# Extract relevant data and reshape\n",
        "test_data_X = torch.tensor(test_mat_data[\"data\"]).reshape(-1, 3, 32, 32).float() / 255.0  # Reshape and normalize\n",
        "test_data_Y = torch.tensor(test_mat_data[\"labels\"]).squeeze()  # Squeeze to remove extra dimension\n",
        "print(test_data_X.shape)\n",
        "# Create TensorDataset\n",
        "test_dataset = TensorDataset(test_data_X, test_data_Y)\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 100  # Adjust batch size as needed\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN_RQ3T9LElT",
        "outputId": "1533b54b-f8dc-4e4a-ee70-1ed503157102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0166, Accuracy: 4209/10000 (42.09%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        # data, target = data.to('cuda'), target.to('cuda')\n",
        "        output = model(data)\n",
        "        test_loss += criterion(output, target).item()\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
